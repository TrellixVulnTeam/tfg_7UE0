\chapter{Conclusiones}
Con la realización de estos experimentos he completado las tareas descritas en la Sección \ref{planificacion}, y por tanto, los objetivos formulados en la Sección \ref{objetivos}. La mejora del balanceo del \textit{dataset} mediante \textit{data augmentation} así como la generación de \textit{hard negatives} ha permitido refinar el conjunto de datos inicial preparándolo para las tareas de clasificación, cumpliendo así con el primer objetivo. 

De igual modo, se han creado dos clasificadores de imágenes que permiten diferenciar aquellas que presentan estructuras de compuestos químicos de las que no (uno entrenado sobre un \textit{dataset} sin \textit{hard negatives} y otro sobre uno con ellos). Esto se ha llevado a cabo aplicando una \textit{grid search}, que ha permitido comparar el rendimiento de distintas arquitecturas e hiperparámetros para este problema, eligiendo los que mejor se comportaban en la creación de los modelos finales. Esto nos ha permitido cumplir el segundo objetivo.

La Inteligencia Artificial, y específicamente el aprendizaje profundo, está cambiando la sociedad. Yann LeCun, Geoffrey Hinton y Yoshua Bengio fueron tres profetas que en la década de los 80 y de los 90 remaron en contra de la comunidad científica, creyendo firmemente que las redes neuronales artificiales eran una herramienta que cambiaría la ciencia. A partir de sus logros, muchos otros científicos se han unido a esta línea de investigación, que es una de las más importantes en Inteligencia Artificial hoy en día. Aun así, todavía queda mucho por hacer y es que aunque se han construido modelos que funcionan muy bien en tareas concretas, no se ha alcanzado esa madurez en modelos que sean capaces de llevar a cabo tareas más genéricas. En este proyecto, de generación y clasificación de imágenes, he trabajado en un ámbito que se encuentra bastante pulido, obteniendo buenos resultados.

Un gran problema del aprendizaje automático en general y del aprendizaje profundo en particular es la explicabilidad de los modelos, ya que funcionan como una caja negra. Por ello, aunque los científicos de la Universidad de Negev me pregunten cómo influye cada parámetro en el resultado final, puedo darles una explicación teórica de cómo afecta al resultado cada parámetro, pero no puedo dar una explicación concreta. En los sistemas expertos basados en reglas esto no ocurre, ya que el conocimiento está codificado de forma explícita. 

Lo más importante en los proyectos de ciencia de datos es la búsqueda y preprocesamiento de la información con la que se van a entrenar los modelos. La predicción y generación que realicen estos modelos dependerá de la calidad del \textit{dataset} de entrenamiento, por lo que es muy importante elegir un conjunto de datos adecuado a la tarea que se va a realizar. Por ejemplo, el \textit{dataset} debe estar balanceado, y los ejemplos que se incluyan deben ser una muestra representativa del problema real a modelar. En las \textit{cheminformatics} el problema del balanceo muchas veces es muy acusado: existen conjuntos de datos donde existe una única muestra para una clase, mientras existen miles para otras clases. Por ello, los científicos que trabajan en este ámbito deben crear modelos acordes con estas limitaciones, donde no siempre es posible aplicar \textit{data augmentation}. 

En este proyecto, obtengo buenos resultados (menos de un 5\% de error). Aunque he realizado numerosas pruebas y esto me ha permitido alcanzar este valor, influye el hecho de que el \textit{dataset} que recibo no sea demasiado complejo. Los ejemplos negativos son imágenes diferenciadas de los ejemplos positivos.

\section{Trabajo futuro}
Tras este proyecto se podrían realizar diferentes mejoras del mismo así como ampliaciones. 

Como mejora se podría intentar crear un conjunto de \textit{hard negatives} más diverso: probar distintos modelos generativos, entrenar sobre otros \textit{datasets} de química, en definitiva, crear imágenes con más diferencias entre sí. También se podría experimentar con distintos ruidos de entrada diferentes al ruido Perlín. 

Para el modelo clasificador, se podrían intentar ajustar más a fondo los hiperparámetros, ya que como hemos comentado para VGG16, el error durante el entrenamiento no decrece de forma suave. Probar con tamaños de minilote mayores (como 128) ayudaría.

Otro apartado a mejorar es el relativo a los tiempos de ejecución y la eficiencia: operaciones como llevar a cabo la \textit{grid search} tardan días incluso ejecutándose en GPU. Actualmente existen bibliotecas como PyTorch Lightning que permiten llevar a cabo entrenamientos de forma distribuida, permitiendo el entrenamiento en múltiples GPUs a la vez.

Como línea de trabajo futura se podría crear un clasificador mucho más avanzado, no solo capaz de distinguir si una imagen de una publicación contiene una estructura molecular o no, sino ser capaz de clasificar diferentes tipos de moléculas. Para ello, se podría construir un \textit{ensemble} de redes neuronales cada una especializada en una función concreta (detectar la presencia de ciertos átomos, enlaces o subestructuras) de forma que trabajasen en equipo para alcanzar el objetivo final.

Finalmente, integrar los modelos finales ya entrenados en una página web sería una buena idea, ya que muchos químicos no tienen conocimientos avanzados de informática. De esta forma podrían subir imágenes/publicaciones y recibirlas clasificadas.
