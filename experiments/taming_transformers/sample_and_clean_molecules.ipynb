{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/homeGPU1/pbedmar/pycharm/experiments/taming_transformers/taming-transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import PIL.ImageShow\n",
    "%cd \"/mnt/homeGPU1/pbedmar/pycharm/experiments/taming_transformers/taming-transformers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f51ed5a0a30>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from functions import clean_molecule, perlin_noise, preprocess_vqgan, preprocess, custom_to_pil, reconstruct_with_vqgan\n",
    "from functions import load_config, load_vqgan\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def sample_perlin_and_clean(config_name, directory, size, amp, freq, cutoff, n, output_path):\n",
    "    config = load_config(\"logs/\"+directory+config_name+\"configs/\"+directory+\"-project.yaml\", display=False)\n",
    "    model = load_vqgan(config, ckpt_path=\"logs/\"+directory+config_name+\"/checkpoints/last.ckpt\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for i in range(n):\n",
    "        noise = perlin_noise(amp, freq, size)\n",
    "        img = Image.fromarray(noise).convert(\"RGB\")\n",
    "        h, w = img.size\n",
    "        img = preprocess(img, h)\n",
    "\n",
    "        gen_img = custom_to_pil(reconstruct_with_vqgan(preprocess_vqgan(img), model)[0])\n",
    "        clean_img = clean_molecule(np.asarray(gen_img), cutoff)\n",
    "        cv2.imwrite(output_path+\"perlin_a\"+str(amp)+\"_f\"+str(freq)+\"_\"+str(i)+\".jpg\",clean_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def sample_uniform_and_clean(config_name, directory, size, cutoff, n, output_path):\n",
    "    config = load_config(\"logs/\"+directory+config_name+\"configs/\"+directory+\"-project.yaml\", display=False)\n",
    "    model = load_vqgan(config, ckpt_path=\"logs/\"+directory+config_name+\"/checkpoints/last.ckpt\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for i in range(n):\n",
    "        noise = torch.rand(1, 3, size, size)\n",
    "\n",
    "        gen_img = custom_to_pil(reconstruct_with_vqgan(preprocess_vqgan(noise), model)[0])\n",
    "        clean_img = clean_molecule(np.asarray(gen_img), cutoff)\n",
    "        cv2.imwrite(output_path+\"uniform\"+\"_\"+str(i)+\".jpg\",clean_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n"
     ]
    }
   ],
   "source": [
    "sample_perlin_and_clean(\"_custom_vqgan_aug_2/\", \"2022-03-24T15-24-33\", size=192, amp=1, freq=2, cutoff=200, n=15, output_path=\"../validation/aug2/clean_70ep/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "Unsqueezed: torch.Size([1, 3, 192, 192])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n"
     ]
    }
   ],
   "source": [
    "sample_perlin_and_clean(\"_custom_vqgan_aug_2/\", \"2022-04-19T21-41-35\", size=192, amp=1, freq=2, cutoff=200, n=15, output_path=\"../validation/aug2/clean_75ep/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([12, 12])\n"
     ]
    }
   ],
   "source": [
    "sample_uniform_and_clean(\"_custom_vqgan_aug_2/\", \"2022-03-24T15-24-33\", size=192, cutoff=210, n=15, output_path=\"../validation/aug2/clean_70ep/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n",
      "Unsqueezed: torch.Size([1, 3, 256, 256])\n",
      "VQGAN --- VQModel: latent shape: torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "sample_perlin_and_clean(\"_custom_vqgan_256_aug_2/\", \"2022-04-17T21-41-00\", size=256, amp=3, freq=2, cutoff=210, n=15, output_path=\"../validation/256/aug2/clean_80ep/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}